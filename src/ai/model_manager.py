"""
Model Manager - Ã‡oklu model desteÄŸi ve otomatik seÃ§im
"""
import os
from pathlib import Path
from typing import Optional, Literal
from llama_cpp import Llama

ModelType = Literal["fast", "accurate", "vision"]

class ModelManager:
    """FarklÄ± gÃ¶revler iÃ§in farklÄ± modeller yÃ¶netir"""
    
    def __init__(self):
        self.models = {}
        self.model_paths = {
            "fast": os.getenv("FAST_MODEL_PATH", "models/phi-3-mini-4k-instruct-q4.gguf"),
            "accurate": os.getenv("ACCURATE_MODEL_PATH", "models/qwen2.5-7b-instruct-q4_k_m-00001-of-00002.gguf"),
            "vision": os.getenv("VISION_MODEL_PATH", "models/llava-v1.5-7b-Q4_K.gguf"),
        }
        
    def get_model(self, model_type: ModelType = "accurate") -> Optional[Llama]:
        """Model yÃ¼kle veya cache'den getir"""
        if model_type in self.models:
            return self.models[model_type]
            
        model_path = self.model_paths.get(model_type)
        if not model_path or not Path(model_path).exists():
            print(f"âš ï¸ {model_type} model bulunamadÄ±: {model_path}")
            # Fallback to accurate model
            if model_type != "accurate":
                return self.get_model("accurate")
            return None
            
        try:
            print(f"ðŸ“¥ {model_type} model yÃ¼kleniyor...")
            
            # GPU desteÄŸi kontrol
            n_gpu_layers = -1 if self._has_gpu() else 0
            
            model = Llama(
                model_path=model_path,
                n_ctx=2048,
                n_threads=8,
                n_gpu_layers=n_gpu_layers,
                verbose=False
            )
            
            self.models[model_type] = model
            print(f"âœ… {model_type} model yÃ¼klendi (GPU: {n_gpu_layers > 0})")
            return model
            
        except Exception as e:
            print(f"âŒ Model yÃ¼kleme hatasÄ±: {e}")
            return None
    
    def _has_gpu(self) -> bool:
        """CUDA/GPU desteÄŸi var mÄ± kontrol et"""
        try:
            import torch
            return torch.cuda.is_available()
        except:
            return False
    
    def select_model_for_task(self, task: str) -> ModelType:
        """GÃ¶rev tipine gÃ¶re en uygun modeli seÃ§"""
        task_lower = task.lower()
        
        # Basit sorular iÃ§in hÄ±zlÄ± model
        simple_keywords = ["kaÃ§", "toplam", "ne kadar", "var mÄ±", "liste"]
        if any(kw in task_lower for kw in simple_keywords):
            return "fast" if Path(self.model_paths["fast"]).exists() else "accurate"
        
        # GÃ¶rsel iÅŸleme
        if "resim" in task_lower or "fotoÄŸraf" in task_lower or "gÃ¶rsel" in task_lower:
            return "vision"
        
        # VarsayÄ±lan: doÄŸru model
        return "accurate"
    
    def clear_cache(self):
        """TÃ¼m modelleri bellekten temizle"""
        self.models.clear()

# Global instance
_model_manager = None

def get_model_manager() -> ModelManager:
    """Singleton model manager"""
    global _model_manager
    if _model_manager is None:
        _model_manager = ModelManager()
    return _model_manager
